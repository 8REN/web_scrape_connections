{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide PATH variable dependent on browser user commands Selenium to use, below is the commented code for chrome\n",
    "# PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "def sign_in(email, password, PATH):\n",
    "#     this function automates login process on linkedin, \n",
    "#     provide email and password as strings\n",
    "    driver = webdriver.Chrome(PATH)\n",
    "    time.sleep(5)\n",
    "    # webdriver gets page\n",
    "    driver.get(\"https://www.linkedin.com/\")\n",
    "    # pause for page to load\n",
    "    time.sleep(3)\n",
    "    # locate and send login email and password\n",
    "    driver.find_element_by_id(\"session_key\").send_keys(email)\n",
    "    driver.find_element_by_id(\"session_password\").send_keys(password)\n",
    "    driver.find_element_by_class_name(\"sign-in-form__submit-button\").click()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_scrape(search_term, num_pages):\n",
    "           # create lists to hold data  \n",
    "    loc_list = []\n",
    "    current_job_list = []\n",
    "    name_list = []\n",
    "    url_list = []\n",
    "    headline_list = []\n",
    "#                 function to automate search bar, search focus\n",
    "#                 from your linkedin homepage, collecting data from\n",
    "#                 search including name, location(secondary_deets), \n",
    "#                 headline(primary_deets) from condensed profiles returned from\n",
    "#                 search results. requires string entry for search term\n",
    "#                 and int input for number of pages (num_pages) \n",
    "#                 that you wish to scrape from results.\n",
    "\n",
    "    # activate search bar cursor with click\n",
    "    driver.find_element_by_css_selector(\"div#global-nav-search \").click()\n",
    "    time.sleep(2)\n",
    "    # send keyboard entry \"div[id='oc-background-section']\")for search terms\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(search_term)\n",
    "    # send enter key to activate search\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(Keys.RETURN)\n",
    "    # wait for results to load\n",
    "    driver.implicitly_wait(6)\n",
    "    #w.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.reusable-search__entity-results-list')))\n",
    "    element = driver.find_element_by_css_selector('ul.reusable-search__entity-results-list ')\n",
    "    # scroll to element containing target(people_banner)  allowing ajax elements to load\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "    driver.implicitly_wait(5)\n",
    "   # locate banner under search type results\n",
    "    people_banner = driver.find_element_by_link_text(f\"See all people results\")\n",
    "    # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',people_banner)\n",
    "    # pause to allow page to load\n",
    "    driver.implicitly_wait(6)\n",
    "    # starting with pagination page 1                       \n",
    "    page_number = 1\n",
    "    # get current url for page\n",
    "    current_page_url = driver.current_url\n",
    "\n",
    "    # set while loop to define pagination and data collection conditions\n",
    "    while page_number <= num_pages:\n",
    "        print(\"Processing page: \" + str(page_number))\n",
    "    # find all results on page\n",
    "        links = driver.find_elements_by_css_selector(\"div.entity-result__content \")\n",
    "   # pause for page load\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # iterate through results\n",
    "        for l in links:\n",
    "       # retrieve profile url\n",
    "            title = l.find_element_by_css_selector(\"span.entity-result__title a.app-aware-link\")\n",
    "            profile_path = (str(title.get_attribute(\"pathname\")))\n",
    "    # add to urls list\n",
    "            url_list.append(profile_path) \n",
    "        \n",
    "    # locating elements containing text needed\n",
    "            details = l.find_elements_by_css_selector(\"div.linked-area\")\n",
    "    # the first element has the first three lines of text in the container\n",
    "            deets = details[0]  \n",
    "    # split text to assign elements appropriately\n",
    "            text = deets.get_attribute('innerText').split('\\n')\n",
    "    # retrieve name/add to list\n",
    "            name = text[0]\n",
    "            name_list.append(name)\n",
    "            \n",
    "    # retrieve location/add to list   \n",
    "            loc = text[-1]\n",
    "            loc_list.append(loc)\n",
    "            \n",
    "     # retrieve headline/add to list\n",
    "            headline = text[-2]\n",
    "            headline_list.append(headline)\n",
    "            \n",
    "#             if len(details) >= 1:\n",
    "#     # the second element selected contains the 'Current:' job text \n",
    "#                 current_job = details[1]\n",
    "#     # removing the 'Current:' string from text\n",
    "#                 current_job = current_job.get_attribute('innerText').split(':')[1]\n",
    "#             else:\n",
    "#                 current_job = 'nan'\n",
    "#     # add current job to list\n",
    "#             current_job_list.append(current_job)\n",
    "        time.sleep(3)\n",
    "    # navigate using pagination function\n",
    "        goto_next_page()\n",
    "        page_number+=1\n",
    "    # print to verify page during processing\n",
    "        print(f\"attempting to navigate to search results page {page_number}\")\n",
    "        time.sleep(5)\n",
    "   \n",
    "    # create dataframe with extracted information and save as csv file\n",
    "    df = pd.DataFrame()                      \n",
    "    df['name'] = name_list\n",
    "    df['url'] = url_list\n",
    "    #df['current_job'] = current_job_list\n",
    "    df['location'] = loc_list\n",
    "    df['headline'] = headline_list\n",
    "    # add complete url information for use in complete profile scraping\n",
    "    for row in df:\n",
    "        df['fetch'] = 'https://www.linkedin.com' + df.url + '/'\n",
    "    df.to_csv(f'{search_term}.csv')\n",
    "    # verify save\n",
    "    print(f'{search_term}.csv saved')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_search_scrape(num_pages, save_label):\n",
    "    loc_list = []\n",
    "    current_job_list = []\n",
    "    name_list = []\n",
    "    url_list = []\n",
    "    headline_list = []\n",
    "        \n",
    "#                 function used when user inputs search manually, collecting data from\n",
    "#                 search including name, location(secondary_deets), \n",
    "#                 headline(primary_deets) from condensed profiles returned from\n",
    "#                 search results. requires string entry for search term\n",
    "#                 and int input for number of pages (num_pages) \n",
    "#                 that you wish to scrape from results.\n",
    "\n",
    "\n",
    "    # starting with pagination page 1                       \n",
    "    page_number = 1\n",
    "\n",
    "    # set while loop to define pagination and data collection conditions\n",
    "    while page_number <= num_pages:\n",
    "    # find all results on page\n",
    "        links = driver.find_elements_by_css_selector(\"div.entity-result__content \")\n",
    "   # pause for page load\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # iterate through results\n",
    "        for l in links:\n",
    "    # retrieve profile url\n",
    "            title = l.find_element_by_css_selector(\"span.entity-result__title a.app-aware-link\")\n",
    "            profile_path = (str(title.get_attribute(\"pathname\")))\n",
    "    # add to urls list\n",
    "            url_list.append(profile_path)\n",
    "            print(profile_path)\n",
    "    # locating elements containing text needed\n",
    "            details = l.find_elements_by_css_selector(\"div.linked-area\")\n",
    "    # the first element has the first three lines of text in the container\n",
    "            deets = details[0]  \n",
    "    # split text to assign elements appropriately\n",
    "            text = deets.get_attribute('innerText').split('\\n')\n",
    "    # retrieve name/add to list\n",
    "            name = text[0]\n",
    "            name_list.append(name)\n",
    "            print(name)\n",
    "    # retrieve location/add to list   \n",
    "            loc = text[-1]\n",
    "            loc_list.append(loc)\n",
    "     # retrieve headline/add to list\n",
    "            headline = text[-2]\n",
    "            headline_list.append(headline)\n",
    "    # the second element selected contains the 'Current:' job text \n",
    "            current_job = details[1]\n",
    "    # removing the 'Current:' string from text\n",
    "            current_job = current_job.get_attribute('innerText').split(':')[1]\n",
    "    # add current job to list\n",
    "            current_job_list.append(current_job)\n",
    "        time.sleep(3)\n",
    "        print(\"Processed page: \" + str(page_number))\n",
    "    # navigate using pagination function\n",
    "        goto_next_page()\n",
    "        page_number+=1\n",
    "    # print to verify page during processing\n",
    "        print(f\"attempting to navigate to search results page {page_number}\")\n",
    "        time.sleep(5)\n",
    "   \n",
    "    # create dataframe with extracted information and save as csv file\n",
    "    df = pd.DataFrame()                      \n",
    "    df['name'] = name_list\n",
    "    df['url'] = url_list\n",
    "    #df['current_job'] = current_job_list\n",
    "    df['location'] = loc_list\n",
    "    df['headline'] = headline_list\n",
    "    # add complete url information for use in complete profile scraping\n",
    "    for row in df:\n",
    "        df['fetch'] = 'https://www.linkedin.com' + df.url + '/'\n",
    "    df.to_csv(f'{save_label}.csv')\n",
    "    # verify save\n",
    "    print(f'{save_label}.csv saved')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to locate and interact with \"next\" button at bottom of search\n",
    "def goto_next_page():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_css_selector('div.artdeco-pagination')\n",
    "    go_to_next = driver.find_element_by_css_selector('button[aria-label=\"Next\"]')\n",
    "    driver.execute_script('arguments[0].click();',go_to_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_experience():\n",
    "#     w = WebDriverWait(driver, 10)\n",
    "#     w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "#     time.sleep(5)\n",
    "#     # background container\n",
    "#     background = driver.find_element_by_css_selector(\n",
    "#         \"div[id='oc-background-section']\")\n",
    "#     # scroll element into view enabling location of elements within for dynamic loading page\n",
    "#     driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "#     time.sleep(8)\n",
    "#     # try/except clause used to locate element, not all people list jobs\n",
    "#     try:\n",
    "#         exp = background.find_element_by_css_selector(\n",
    "#                 \"section#experience-section.pv-profile-section.experience-section\")\n",
    "    \n",
    "#     except (NoSuchElementException, IndexError):\n",
    "#         pos = 'nan'\n",
    "#         company= 'nan'\n",
    "#         location = 'nan'\n",
    "#         dates='nan'\n",
    "#     history = exp.find_elements_by_css_selector('li.pv-entity__position-group-pager')\n",
    "#     details = history[0]\n",
    "#     # try/except clause used to locate element to avoid error ceasing from running\n",
    "#     try:\n",
    "#         pos = details.find_element_by_tag_name(\n",
    "#                 'h3').get_attribute('outerText')\n",
    "#     except NoSuchElementException:\n",
    "#         pos = 'nan'\n",
    "\n",
    "#     try:\n",
    "#         company = details.find_element_by_tag_name(\n",
    "#                 'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "        \n",
    "#     except NoSuchElementException:\n",
    "#         company = 'nan'\n",
    "#     try:\n",
    "#         location = details.find_element_by_css_selector(\n",
    "#                 'h4.pv-entity__location').get_attribute('innerText')\n",
    "#         location = location.split('\\n', 1)[1]\n",
    "#     except NoSuchElementException:\n",
    "#         location = 'nan'\n",
    "        \n",
    "#     try:\n",
    "#         dates = details.find_element_by_css_selector(\n",
    "#                 \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "\n",
    "#     except NoSuchElementException:\n",
    "#             dates = 'nan'\n",
    "#     finally:\n",
    "#         job_list.append(pos)\n",
    "#         company_list.append(company)\n",
    "#         date_list.append(dates)\n",
    "#         loc_list.append(location)\n",
    "\n",
    "#     if (len(history)) >=2:\n",
    "#         details = history[1]\n",
    "        \n",
    "#         # job title\n",
    "#         try:\n",
    "#             pos = details.find_element_by_tag_name(\n",
    "#                     'h3').get_attribute('outerText')\n",
    "#         except NoSuchElementException:\n",
    "#             pos = 'nan'\n",
    "            \n",
    "#         # company of employment\n",
    "#         try:\n",
    "#             company = details.find_element_by_tag_name(\n",
    "#                     'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "#         except NoSuchElementException:\n",
    "#             company = 'nan'\n",
    "            \n",
    "#         # location of employment\n",
    "#         try:\n",
    "#             location = details.find_element_by_css_selector(\n",
    "#                     'h4.pv-entity__location').get_attribute('innerText')\n",
    "#             location = location.split('\\n', 1)[1]\n",
    "#         except NoSuchElementException:\n",
    "#             location = 'nan'\n",
    "            \n",
    "#        # dates of employment\n",
    "#         try:\n",
    "#             dates = details.find_element_by_css_selector(\n",
    "#                     \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "\n",
    "#         except NoSuchElementException:\n",
    "#                 dates = 'nan'\n",
    "\n",
    "#         if (len(history)) < 2:\n",
    "#             pos = 'nan'\n",
    "#             company  ='nan'\n",
    "#             location  ='nan'\n",
    "#             dates = 'nan'\n",
    "\n",
    "#         job_list2.append(pos)\n",
    "#         company_list2.append(company)\n",
    "#         date_list2.append(dates)\n",
    "#         loc_list2.append(location)\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    \n",
    "    \n",
    "#     time.sleep(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience():\n",
    "    job_list = []\n",
    "    company_list = []\n",
    "    location_list = []\n",
    "    date_list = []\n",
    "#     job_list2 = []\n",
    "#     company_list2 = []\n",
    "#     loc_list2 = []\n",
    "#     date_list2 = []\n",
    "    time.sleep(3)\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h2.pv-profile-section__card-heading\")))\n",
    "    h2_heading = driver.find_element_by_css_selector(\n",
    "        \"h2.pv-profile-section__card-heading\")\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", h2_heading)\n",
    "    time.sleep(3)\n",
    "    # background container\n",
    "    background = driver.find_element_by_css_selector(\n",
    "        \"div[id='oc-background-section']\")\n",
    "    # scroll background container into view to access elements within\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "    # wait for section to be accessible\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"section#experience-section\")))\n",
    "    # locate element and access it\n",
    "    test = background.find_elements_by_css_selector(\n",
    "                \"section#experience-section.pv-profile-section.experience-section\")\n",
    "    if test >=1:\n",
    "        exp = background.find_element_by_css_selector(\n",
    "                \"section#experience-section.pv-profile-section.experience-section\")\n",
    "\n",
    "        history = exp.find_elements_by_css_selector('li.pv-entity__position-group-pager')\n",
    "        num_jobs = len(history)\n",
    "        job_count_list.append(num_jobs)\n",
    "        details = history[0]\n",
    "        # try/except clause used to locate element to avoid 'element not found' error which halts program\n",
    "        # job title\n",
    "        try:\n",
    "            pos = details.find_element_by_tag_name(\n",
    "                        'h3').get_attribute('innerText')\n",
    "        except NoSuchElementException:\n",
    "            pos = 'nan'\n",
    "\n",
    "            # company of employment\n",
    "        try:\n",
    "            company = details.find_element_by_tag_name(\n",
    "                        'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "        except NoSuchElementException:\n",
    "            company = 'nan'\n",
    "\n",
    "            # location of employment\n",
    "        try:\n",
    "            location = details.find_element_by_css_selector(\n",
    "                        'h4.pv-entity__location').get_attribute('innerText')\n",
    "            location = location.split('\\n', 1)[1]\n",
    "        except NoSuchElementException:\n",
    "            location = 'nan'\n",
    "\n",
    "           # dates of employment\n",
    "        try:\n",
    "            dates = details.find_element_by_css_selector(\n",
    "                        \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "\n",
    "        except NoSuchElementException:\n",
    "                    dates = 'nan'\n",
    "    else:\n",
    "        pos = 'nan'\n",
    "        company= 'nan'\n",
    "        location = 'nan'\n",
    "        dates='nan'\n",
    "        # save to lists to create dataframe        \n",
    "    job_list.append(pos)\n",
    "    company_list.append(company)\n",
    "    date_list.append(dates)\n",
    "    location_list.append(location)\n",
    "\n",
    "    #     # for second job data retrieval\n",
    "    #     if (len(history)) >=2:\n",
    "    #         details = history[1]\n",
    "\n",
    "#         # job title\n",
    "#         try:\n",
    "#             pos = details.find_element_by_tag_name(\n",
    "#                     'h3').get_attribute('outerText')\n",
    "#         except NoSuchElementException:\n",
    "#             pos = 'nan'\n",
    "            \n",
    "#         # company of employment\n",
    "#         try:\n",
    "#             company = details.find_element_by_tag_name(\n",
    "#                     'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "#         except NoSuchElementException:\n",
    "#             company = 'nan'\n",
    "            \n",
    "#         # location of employment\n",
    "#         try:\n",
    "#             location = details.find_element_by_css_selector(\n",
    "#                     'h4.pv-entity__location').get_attribute('innerText')\n",
    "#             location = location.split('\\n', 1)[1]\n",
    "#         except NoSuchElementException:\n",
    "#             location = 'nan'\n",
    "            \n",
    "#        # dates of employment\n",
    "#         try:\n",
    "#             dates = details.find_element_by_css_selector(\n",
    "#                     \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "\n",
    "#         except NoSuchElementException:\n",
    "#                 dates = 'nan'\n",
    "\n",
    "#         if (len(history)) < 2:\n",
    "#             pos = 'nan'\n",
    "#             company  ='nan'\n",
    "#             location  ='nan'\n",
    "#             dates = 'nan'\n",
    "\n",
    "#         job_list2.append(pos)\n",
    "#         company_list2.append(company)\n",
    "#         date_list2.append(dates)\n",
    "#         loc_list2.append(location)\n",
    "        \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    \n",
    "    time.sleep(2)\n",
    "    return job_list, company_list, date_list, location_list, job_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_education():\n",
    "    school_list = []\n",
    "        # function accessing and saves profile school information, optional second job retrieval commented out below\n",
    "    # wait for element to proceed\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "    # background container\n",
    "    try:\n",
    "        background = driver.find_element_by_css_selector(\n",
    "            \"div[id='oc-background-section']\")\n",
    "    # scroll into view to access elements within\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "        time.sleep(8)\n",
    "    # locate education section\n",
    "        education = background.find_element_by_css_selector(\n",
    "            \"section[id='education-section']\")\n",
    "        schools = education.find_elements_by_css_selector(\n",
    "                \"a[data-control-name='background_details_school']\")\n",
    "        school = schools[0]\n",
    "        school_name = school.find_element_by_tag_name(\n",
    "            'h3').get_attribute('innerText')\n",
    "    except NoSuchElementException:\n",
    "        school_name = 'nan'\n",
    "\n",
    "    school_list.append(school_name)\n",
    "    return school_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email():\n",
    "#     function accessing and retrieves email and name from profile header, extracts first name \n",
    "    names_list = []\n",
    "    fname_list = []\n",
    "    email_list = []\n",
    "    # scroll to top of profile where email is located\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    header = driver.find_elements_by_css_selector(\n",
    "        \"ul.pv-top-card--list\")\n",
    "    name = header[0].get_attribute('innerText')\n",
    "    names_list.append(name)\n",
    "    fname = name.split(' ')\n",
    "    first_name = name[0]\n",
    "    fname_list.append(first_name)\n",
    "    contact_info = driver.find_element_by_css_selector(\n",
    "        'a[data-control-name=\"contact_see_more\"]')\n",
    "     # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',contact_info)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        container =  driver.find_elements_by_css_selector(\n",
    "            'div.pv-contact-info__ci-container')\n",
    "        email =  container[1].get_attribute('innerText')\n",
    "     \n",
    "    \n",
    "    except (NoSuchElementException, IndexError):\n",
    "        email = 'nan'\n",
    "    \n",
    "    email_list.append(email)\n",
    "    close_popup = driver.find_element_by_css_selector(\n",
    "            'button[aria-label=\"Dismiss\" ]')\n",
    "    close_popup.click()\n",
    "    return names_list, fname_list, email_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_connect(message):\n",
    "    # function used to interact with profile and send personalized message and connection request\n",
    "    # scroll to top of profile to ensure elements can be found by webdriver\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(5)\n",
    "    # locate & click 'Connect' button located at top of profile\n",
    "    driver.find_element_by_class_name('pv-s-profile-actions--connect').click()\n",
    "    time.sleep(4)\n",
    "    action_bar = driver.find_element_by_css_selector(\"div.artdeco-modal__actionbar\")\n",
    "    # locate 'Add a note' button by class\n",
    "    action_bar.find_element_by_class_name('mr1').click()\n",
    "    time.sleep(3)\n",
    "    # action to send text from message to input box\n",
    "    message_input = driver.find_element_by_id('custom-message').send_keys(message)\n",
    "    # send the message & connection request\n",
    "    message_input.find_element_by_id('custom-message').send_keys(Keys.RETURN)\n",
    "    # send the message & connection request\n",
    "    # driver.find_element_by_class_name('ml1').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_scrape(profile_urls):\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        get_experience()\n",
    "        get_education()\n",
    "        get_email()\n",
    "    df = pd.DataFrame()\n",
    "    df['first_name'] = fname_list\n",
    "    df['email'] = email_list\n",
    "    df['profile_name'] = names_list\n",
    "    df['job1'] = job_list\n",
    "    df['company1'] = company_list\n",
    "    df['location1'] = loc_list\n",
    "    df['dates1'] = date_list\n",
    "    df['number of jobs'] = job_count_list\n",
    "#     df['job2'] = job_list2\n",
    "#     df['company2'] = company_list2\n",
    "#     df['location2'] = loc_list2\n",
    "#     df['dates2'] = date_list2\n",
    "#     df['school'] = school_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_connection(url_message_dict):\n",
    "    counter = 1\n",
    "\n",
    "    for key, value in url_message_dict.items():\n",
    "        url = key\n",
    "        message = value\n",
    "        driver.get(url)\n",
    "        profile_connect(message)\n",
    "        print('connection to : ' +url+ ' is complete')\n",
    "        counter+=1\n",
    "    print(counter+' connections successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_scrape_connect(search_term, num_pages):\n",
    "\n",
    "    search_df = people_scrape(search_term, num_pages)\n",
    "\n",
    "    profile_urls = search_df['fetch']\n",
    "   \n",
    "    detail_df = profile_scrape(profile_urls)\n",
    "\n",
    "    df = pd.concat([search_df, detail_df], axis=1)\n",
    "    df = df.loc[df.company1 != 'nan']\n",
    "    df = df.loc[df.job1 != 'nan']\n",
    "  \n",
    "    # each user to create message string with format options\n",
    "    df['personalized_message'] = (\"Hi \" + df.first_name +\", my name is Brennan, and I am a data scientist/ml engineer in the DMV area.\"\n",
    "                                                       \"My background is in video editing and QA. I see that you work as a \" + df.job1 + \"at \"+ df.company1+\n",
    "                                                       \", which sounds super interesting, so I just wanted to reach out, connect, and say hello!\")  \n",
    "    message_inputs = list(df['personalized_message'])\n",
    "    urls = list(df['fetch'])\n",
    "    connection_dict = dict(zip(urls, message_inputs))\n",
    "    df.to_csv(search_term+'_comp.csv')\n",
    "    make_connection(connection_dict)\n",
    "    return df\n",
    "    #driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
