{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from login import sign_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_in(email, password):\n",
    "#     this function automates login process on linkedin, \n",
    "#     provide email and password as strings\n",
    "    # webdriver gets page\n",
    "    driver.get(\"https://www.linkedin.com/\")\n",
    "    # pause for page to load\n",
    "    time.sleep(3)\n",
    "    # locate and send login email and password\n",
    "    driver.find_element_by_id(\"session_key\").send_keys(email)\n",
    "    driver.find_element_by_id(\"session_password\").send_keys(password)\n",
    "    driver.find_element_by_class_name(\"sign-in-form__submit-button\").click()\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_scrape(search_term, num_pages):\n",
    "        \n",
    "#                 function to automate search bar, search focus\n",
    "#                 from your linkedin homepage, collecting data from\n",
    "#                 search including name, location(secondary_deets), \n",
    "#                 headline(primary_deets) from condensed profiles returned from\n",
    "#                 search results. requires string entry for search term\n",
    "#                 and int input for number of pages (num_pages) \n",
    "#                 that you wish to scrape from results.\n",
    "\n",
    "    # activate search bar cursor with click\n",
    "    driver.find_element_by_css_selector(\"div#global-nav-search \").click()\n",
    "    time.sleep(2)\n",
    "    # send keyboard entry \"div[id='oc-background-section']\")for search terms\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(search_term)\n",
    "    # send enter key to activate search\n",
    "    driver.find_element_by_css_selector(\"input.search-global-typeahead__input\").send_keys(Keys.RETURN)\n",
    "    # wait for results to load\n",
    "    driver.implicitly_wait(6)\n",
    "    #w.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.reusable-search__entity-results-list')))\n",
    "    element = driver.find_element_by_css_selector('ul.reusable-search__entity-results-list ')\n",
    "    # scroll to element containing target(people_banner)  allowing ajax elements to load\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "    driver.implicitly_wait(5)\n",
    "   # locate banner under search type results\n",
    "    people_banner = driver.find_element_by_link_text(f\"See all people results\")\n",
    "    # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',people_banner)\n",
    "    # pause to allow page to load\n",
    "    driver.implicitly_wait(6)\n",
    "    # starting with pagination page 1                       \n",
    "    page_number = 1\n",
    "    # get current url for page\n",
    "    current_page_url = driver.current_url\n",
    "\n",
    "    # set while loop to define pagination and data collection conditions\n",
    "    while page_number <= num_pages:\n",
    "        print(\"Processing page: \" + str(page_number))\n",
    "    # find all results on page\n",
    "        links = driver.find_elements_by_css_selector(\"div.entity-result__content \")\n",
    "   # pause for page load\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # iterate through results\n",
    "        for l in links:\n",
    "    # retrieve profile url\n",
    "            title = l.find_element_by_css_selector(\"span.entity-result__title a.app-aware-link\")\n",
    "            profile_path = (str(title.get_attribute(\"pathname\")))\n",
    "    # add to urls list\n",
    "            url_list.append(profile_path)\n",
    "    # locating elements containing text needed\n",
    "            details = l.find_elements_by_css_selector(\"div.linked-area\")\n",
    "    # the first element has the first three lines of text in the container\n",
    "            deets = details[0]  \n",
    "    # split text to assign elements appropriately\n",
    "            text = deets.get_attribute('innerText').split('\\n')\n",
    "    # retrieve name/add to list\n",
    "            name = text[0]\n",
    "            names_list.append(name)\n",
    "    # retrieve location/add to list   \n",
    "            loc = text[-1]\n",
    "            loc_list.append(loc)\n",
    "     # retrieve headline/add to list\n",
    "            headline = text[-2]\n",
    "            headline_list.append(headline)\n",
    "    # the second element selected contains the 'Current:' job text \n",
    "            current_job = details[1]\n",
    "    # removing the 'Current:' string from text\n",
    "            current_job = current_job.get_attribute('innerText').split(':')[1]\n",
    "    # add current job to list\n",
    "            current_job_list.append(current_job)\n",
    "        time.sleep(3)\n",
    "    # navigate using pagination function\n",
    "        goto_next_page()\n",
    "        page_number+=1\n",
    "    # print to verify page during processing\n",
    "        print(f\"attempting to navigate to search results page {page_number}\")\n",
    "        time.sleep(5)\n",
    "   \n",
    "    # create dataframe with extracted information and save as csv file\n",
    "    df = pd.DataFrame()                      \n",
    "    df['name'] = names_list\n",
    "    df['url'] = url_list\n",
    "    df['current_job'] = current_job_list\n",
    "    df['location'] = loc_list\n",
    "    df['headline'] = headline_list\n",
    "    # add complete url information for use in complete profile scraping\n",
    "    for row in df:\n",
    "        df['fetch'] = 'https://www.linkedin.com' + df.url + '/'\n",
    "    df.to_csv(f'{search_term}.csv')\n",
    "    # verify save\n",
    "    print(f'{search_term}.csv saved')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_name():\n",
    "    profile_header = driver.find_element_by_css_selector(\"ul.pv-top-card--list\")\n",
    "    # find name element from profile header portion\n",
    "    header_details = profile_header.find_elements_by_tag_name(\"li\")\n",
    "    profile_name = header_details[0].get_attribute('innerText')\n",
    "    return profile_name\n",
    "\n",
    "def get_first_name():\n",
    "    full_name = get_name()\n",
    "    name = full_name.split(' ', 1)\n",
    "    first_name = name[0]\n",
    "    return first_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to locate and interact with \"next\" button at bottom of search\n",
    "def goto_next_page():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(7)\n",
    "    driver.find_element_by_css_selector('div.artdeco-pagination.ember-view')\n",
    "    go_to_next = driver.find_element_by_css_selector('button[aria-label=\"Next\"]')\n",
    "    driver.execute_script('arguments[0].click();',go_to_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience():\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "    time.sleep(5)\n",
    "    # background container\n",
    "    background = driver.find_element_by_css_selector(\n",
    "        \"div[id='oc-background-section']\")\n",
    "    #print(\"here\" + str(background))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "    \n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"section#experience-section\")))\n",
    "    # try/except clause used to locate element, not all people list jobs\n",
    "    exp = background.find_element_by_css_selector(\n",
    "            \"section[id='experience-section']\")\n",
    "    history = exp.find_elements_by_css_selector('li.pv-entity__position-group-pager')\n",
    "    details = history[0]\n",
    "    # try/except clause used to locate element to avoid 'element not found' error ceasing from running\n",
    "    try:\n",
    "        pos = details.find_element_by_tag_name(\n",
    "                'h3').get_attribute('outerText')\n",
    "    except NoSuchElementException:\n",
    "        pos = 'nan'\n",
    "\n",
    "    try:\n",
    "        company = details.find_element_by_tag_name(\n",
    "                'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "    except NoSuchElementException:\n",
    "        company = 'nan'\n",
    "        \n",
    "    try:\n",
    "        location = details.find_element_by_css_selector(\n",
    "                'h4.pv-entity__location').get_attribute('innerText')\n",
    "        location = location.split('\\n', 1)[1]\n",
    "    except NoSuchElementException:\n",
    "        location = 'nan'\n",
    "        \n",
    "    try:\n",
    "        dates = details.find_element_by_css_selector(\n",
    "                \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "    except NoSuchElementException:\n",
    "            dates = 'nan'\n",
    "    \n",
    "    job_list.append(pos)\n",
    "    company_list.append(company)\n",
    "    date_list.append(dates)\n",
    "    loc_list.append(location)\n",
    "\n",
    "    if (len(history)) >=2:\n",
    "        details = history[1]\n",
    "        \n",
    "        try:\n",
    "            pos = details.find_element_by_tag_name(\n",
    "                    'h3').get_attribute('outerText')\n",
    "        except NoSuchElementException:\n",
    "            pos = 'nan'\n",
    "     \n",
    "        try:\n",
    "            company = details.find_element_by_tag_name(\n",
    "                    'p.pv-entity__secondary-title').get_attribute('innerText')\n",
    "        except NoSuchElementException:\n",
    "            company = 'nan'\n",
    "            \n",
    "        try:\n",
    "            location = details.find_element_by_css_selector(\n",
    "                    'h4.pv-entity__location').get_attribute('innerText')\n",
    "            location = location.split('\\n', 1)[1]\n",
    "        except NoSuchElementException:\n",
    "            location = 'nan'\n",
    "                #print(location)\n",
    "        try:\n",
    "            dates = details.find_element_by_css_selector(\n",
    "                    \"h4.pv-entity__date-range\").get_attribute('innerText').split(' ', 2)[-1]\n",
    "\n",
    "        except NoSuchElementException:\n",
    "                dates = 'nan'\n",
    "\n",
    "        if (len(history)) < 2:\n",
    "            pos = 'nan'\n",
    "            company  ='nan'\n",
    "            location  ='nan'\n",
    "            dates = 'nan'\n",
    "\n",
    "        job_list2.append(pos)\n",
    "        company_list2.append(company)\n",
    "        date_list2.append(dates)\n",
    "        loc_list2.append(location)\n",
    "        \n",
    "        \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_education():\n",
    "    # wait for element to proceed\n",
    "    w = WebDriverWait(driver, 10)\n",
    "    w.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"div.profile-detail\")))\n",
    "    # background container\n",
    "    background = driver.find_element_by_css_selector(\n",
    "        \"div[id='oc-background-section']\")\n",
    "    #print(\"here\" + str(background))\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView();\", background)\n",
    "    time.sleep(8)\n",
    "    # locate education section\n",
    "    education = background.find_element_by_css_selector(\n",
    "            \"section[id='education-section']\")\n",
    " \n",
    "    try:\n",
    "        schools = education.find_elements_by_css_selector(\n",
    "                \"a[data-control-name='background_details_school']\")\n",
    "        school = schools[0]\n",
    "        school_name = school.find_element_by_tag_name(\n",
    "            'h3').get_attribute('innerText')\n",
    "    except NoSuchElementException:\n",
    "        school_name =  'nan'\n",
    "\n",
    "    school_list.append(school_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email():\n",
    "    # scroll to top of profile where email is located\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    contact_info = driver.find_element_by_css_selector(\n",
    "        'a[data-control-name=\"contact_see_more\"]')\n",
    "     # js function to click banner/button to see additional results under jobs, people, or posts                      \n",
    "    driver.execute_script('arguments[0].click();',contact_info)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        container =  driver.find_elements_by_css_selector(\n",
    "            'div.pv-contact-info__ci-container')\n",
    "        email =  container[1].get_attribute('innerText')\n",
    "     \n",
    "    \n",
    "    except (NoSuchElementException, IndexError):\n",
    "        email = 'nan'\n",
    "    \n",
    "    email_list.append(email)\n",
    "    close_popup = driver.find_element_by_css_selector(\n",
    "            'button[aria-label=\"Dismiss\" ]')\n",
    "    close_popup.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_connect(message):\n",
    "    # scroll to top of profile to ensure elements can be found by webdriver\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    # locate & click 'Connect' button located at top of profile\n",
    "    driver.find_element_by_class_name('pv-s-profile-actions--connect').click()\n",
    "    action_bar = driver.find_element_by_css_selector(\"div.artdeco-modal__actionbar\")\n",
    "    # locate 'Add a note' button by class\n",
    "    action_bar.find_element_by_class_name('mr1').click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    # action to send text from message to input box\n",
    "    message_input = driver.find_element_by_id('custom-message').send_keys(message)\n",
    "    # send the message & connection request\n",
    "    driver.find_element_by_class_name('ml1').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = sign_in('mathis.brennan@gmail.com', 'Shitballs1!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "email_list = []\n",
    "position_list = []\n",
    "company_list = []\n",
    "location_list = []\n",
    "dates_list = []\n",
    "position_list2 = []\n",
    "company_list2 = []\n",
    "location_list2 = []\n",
    "dates_list2 = []\n",
    "school_list = []\n",
    "school_list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loc_list = []\n",
    "current_job_list = []\n",
    "names_list = []\n",
    "url_list = []\n",
    "headline_list = []\n",
    "\n",
    "#search_df = people_scrape('data analyst', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoconnect(term, pages):\n",
    "    loc_list = []\n",
    "    current_job_list = []\n",
    "    names_list = []\n",
    "    url_list = []\n",
    "    headline_list = []\n",
    "\n",
    "    name_list = []\n",
    "    email_list = []\n",
    "    job_list = []\n",
    "    company_list = []\n",
    "    loc_list = []\n",
    "    date_list = []\n",
    "    job_list2 = []\n",
    "    company_list2 = []\n",
    "    loc_list2 = []\n",
    "    date_list2 = []\n",
    "    school_list = []\n",
    "    \n",
    "    search_df = people_scrape('data scientist ibm', 2)\n",
    "    profile_urls = search_df['fetch']\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        get_experience()\n",
    "        get_education()\n",
    "        get_email()\n",
    "        \n",
    "    detail_df = pd.DataFrame()\n",
    "    detail_df['job1'] = job_list\n",
    "    detail_df['company1'] = company_list\n",
    "    detail_df['location1'] = loc_list\n",
    "    detail_df['dates1'] = date_list\n",
    "    detail_df['job2'] = job_list2\n",
    "    detail_df['company2'] = company_list2\n",
    "    detail_df['location2'] = loc_list2\n",
    "    detail_df['dates2'] = date_list2\n",
    "    detail_df['school'] = school_list\n",
    "    detail_df['email'] = email_list\n",
    "    df = pd.concat([search_df, detail_df], axis=1)\n",
    "    df['first_name'] = df['name'].map(lambda x: x.split(' ')[0])\n",
    "    df['personalized_message'] = (f\"\"\"Hi + {df['first_name']}, my name is Brennan, and I am a data scientist with a background in video.\n",
    "                                  I see that you are a {df['job1']} at {df['company1']}. I am super interested in finding out more\n",
    "                                  about what you do in your current position, so I just wanted to reach out, connect, and say hello!\"\"\")\n",
    "    message_input = list(df['personalized_message'])\n",
    "    counter = 0\n",
    "\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        message = str(message_input[counter])\n",
    "        #profile_connect(message)\n",
    "        print(message)\n",
    "        counter+=1\n",
    "        print(\"You're now connected to:  \" + df['name'] + \" | \" + df['company1'] + \" | \" + df['job1'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page: 1\n",
      "attempting to navigate to search results page 2\n",
      "Processing page: 2\n",
      "attempting to navigate to search results page 3\n",
      "data scientist ibm.csv saved\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'job_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-e251e0bfd615>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mautoconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data engineer ibm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-72156c80916b>\u001b[0m in \u001b[0;36mautoconnect\u001b[1;34m(term, pages)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprofile_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mget_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mget_education\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mget_email\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-2faab10f218b>\u001b[0m in \u001b[0;36mget_experience\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'nan'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mjob_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mcompany_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mdate_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'job_list' is not defined"
     ]
    }
   ],
   "source": [
    "autoconnect('data engineer ibm', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_urls = search_df['fetch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "    name_list = []\n",
    "    email_list = []\n",
    "    job_list = []\n",
    "    company_list = []\n",
    "    loc_list = []\n",
    "    date_list = []\n",
    "    job_list2 = []\n",
    "    company_list2 = []\n",
    "    loc_list2 = []\n",
    "    date_list2 = []\n",
    "    school_list = []\n",
    "\n",
    "    \n",
    "def scrape_profile(profile_urls):\n",
    "    profile_urls = search_df['fetch']\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        get_experience()\n",
    "        get_education()\n",
    "        get_email()\n",
    "        \n",
    "    detail_df = pd.DataFrame()\n",
    "    detail_df['job1'] = job_list\n",
    "    detail_df['company1'] = company_list\n",
    "    detail_df['location1'] = loc_list\n",
    "    detail_df['dates1'] = date_list\n",
    "    detail_df['job2'] = job_list2\n",
    "    detail_df['company2'] = company_list2\n",
    "    detail_df['location2'] = loc_list2\n",
    "    detail_df['dates2'] = date_list2\n",
    "    detail_df['school'] = school_list\n",
    "    detail_df['email'] = email_list\n",
    "    df = pd.concat([search_df, detail_df], axis=1)\n",
    "    df['first_name'] = df['name'].map(lambda x: x.split(' ')[0])\n",
    "    df['personalized_message'] = \"Hi \" + df['first_name'] \n",
    "                                    + \", my name is Brennan, and I am a data scientist looking \"\n",
    "                                    + \"to segue into data engineering. I see that you are a \"\n",
    "                                    + df['job1'] + \" at \" + df['company1']+\". I am super interested in finding out more\"\n",
    "                                    + \" about what you do in your current position, so I just wanted to reach out, connect, and say hello!\"\n",
    "    message_input = list(df['personalized_message'])\n",
    "    \n",
    "    \n",
    "def make_connection():\n",
    "    counter = 0\n",
    "\n",
    "    for url in profile_urls:\n",
    "        driver.get(url)\n",
    "        message = str(message_input[counter])\n",
    "        profile_connect(message)\n",
    "\n",
    "\n",
    "        counter+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-421947df2208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m print(\"Hi \" + df['first_name']\n\u001b[0m\u001b[0;32m      2\u001b[0m                                     \u001b[1;33m+\u001b[0m \u001b[1;34m\", my name is Brennan, and I am a data scientist looking \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                     \u001b[1;33m+\u001b[0m \u001b[1;34m\"to segue into data engineering. I see that you are a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                     \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'job1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" at \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'company1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\". I am super interested in finding out more\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                     + \" about what you do in your current position, so I just wanted to reach out, connect, and say hello!\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Hi \" + df['first_name']\n",
    "                                    + \", my name is Brennan, and I am a data scientist looking \"\n",
    "                                    + \"to segue into data engineering. I see that you are a \"\n",
    "                                    + df['job1'] + \" at \" + df['company1']+\". I am super interested in finding out more\"\n",
    "                                    + \" about what you do in your current position, so I just wanted to reach out, connect, and say hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-856a2c2a8c72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_info_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-115-5f7b08b5ad0c>\u001b[0m in \u001b[0;36mget_info_df\u001b[1;34m(profile_urls)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprofile_urls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mget_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mget_education\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mget_email\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-04ced7fe7971>\u001b[0m in \u001b[0;36mget_experience\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mdetail_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'location2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloc_list2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mdetail_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dates2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate_list2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mdetail_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'school'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschool_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetail_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3162\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3237\u001b[0m         \"\"\"\n\u001b[0;32m   3238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3239\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3240\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3895\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3896\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3897\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3898\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    749\u001b[0m     \"\"\"\n\u001b[0;32m    750\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    752\u001b[0m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (0) does not match length of index (2)"
     ]
    }
   ],
   "source": [
    "get_info_df(profile_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_df = pd.DataFrame()\n",
    "detail_df['job1'] = job_list\n",
    "detail_df['company1'] = company_list\n",
    "detail_df['location1'] = loc_list\n",
    "detail_df['dates1'] = date_list\n",
    "detail_df['job2'] = job_list2\n",
    "detail_df['company2'] = company_list2\n",
    "detail_df['location2'] = loc_list2\n",
    "detail_df['dates2'] = date_list2\n",
    "detail_df['school'] = school_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([search_df, detail_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_name'] = df['name'].map(lambda x: x.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>current_job</th>\n",
       "      <th>location</th>\n",
       "      <th>headline</th>\n",
       "      <th>fetch</th>\n",
       "      <th>job1</th>\n",
       "      <th>company1</th>\n",
       "      <th>location1</th>\n",
       "      <th>dates1</th>\n",
       "      <th>job2</th>\n",
       "      <th>company2</th>\n",
       "      <th>location2</th>\n",
       "      <th>dates2</th>\n",
       "      <th>school</th>\n",
       "      <th>first_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Murali Subramanian</td>\n",
       "      <td>/in/musubr0298</td>\n",
       "      <td>Data Engineer at Searce Inc</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>Data Engineer at Searce Inc. | 2x GCP | 2x AWS</td>\n",
       "      <td>https://www.linkedin.com/in/musubr0298/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Slalom Build Full-time</td>\n",
       "      <td>Chicago, Illinois, United States</td>\n",
       "      <td>2021 – Present</td>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>BrainCo Internship</td>\n",
       "      <td>nan</td>\n",
       "      <td>2020 – Feb 2021</td>\n",
       "      <td>The George Washington University</td>\n",
       "      <td>Murali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jianfang Chen</td>\n",
       "      <td>/in/jianfang-chen-2286a31b1</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>United States</td>\n",
       "      <td>Data Engineer at Microsoft</td>\n",
       "      <td>https://www.linkedin.com/in/jianfang-chen-2286...</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Square</td>\n",
       "      <td>nan</td>\n",
       "      <td>2021 – Present</td>\n",
       "      <td>Company Name\\nZoomSystems</td>\n",
       "      <td>nan</td>\n",
       "      <td>San Francisco Bay Area</td>\n",
       "      <td>2019 – Feb 2021</td>\n",
       "      <td>Hult International Business School</td>\n",
       "      <td>Jianfang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vina Zhu</td>\n",
       "      <td>/in/vina-zhu-4986451a4</td>\n",
       "      <td>Data Engineer at Slalom Build</td>\n",
       "      <td>Arlington, VA</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>https://www.linkedin.com/in/vina-zhu-4986451a4/</td>\n",
       "      <td>Company Name\\nFannie Mae</td>\n",
       "      <td>nan</td>\n",
       "      <td>Reston, Virginia, United States</td>\n",
       "      <td>2020 – Present</td>\n",
       "      <td>Assitant Manager</td>\n",
       "      <td>Papa John's International</td>\n",
       "      <td>nan</td>\n",
       "      <td>2014 – Jul 2016</td>\n",
       "      <td>James Madison University</td>\n",
       "      <td>Vina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Palak Kaur</td>\n",
       "      <td>/in/palak-kaur</td>\n",
       "      <td>A Data Analyst and Engineer taking care of da...</td>\n",
       "      <td>Greater Seattle Area</td>\n",
       "      <td>Data Engineer at Square</td>\n",
       "      <td>https://www.linkedin.com/in/palak-kaur/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Albertsons Companies</td>\n",
       "      <td>Greater Phoenix Area</td>\n",
       "      <td>2021 – Present</td>\n",
       "      <td>Company Name\\nCiti</td>\n",
       "      <td>nan</td>\n",
       "      <td>Irving, Texas, United States</td>\n",
       "      <td>2020 – Jan 2021</td>\n",
       "      <td>Southern Arkansas University</td>\n",
       "      <td>Palak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alper Tosun</td>\n",
       "      <td>/in/alper-tosun-bba82467</td>\n",
       "      <td>Data Management Analyst at Fannie Mae - ...th...</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Engineer at Fannie Mae</td>\n",
       "      <td>https://www.linkedin.com/in/alper-tosun-bba82467/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Cognizant Full-time</td>\n",
       "      <td>nan</td>\n",
       "      <td>2020 – Present</td>\n",
       "      <td>Data Science Program</td>\n",
       "      <td>Thinkful</td>\n",
       "      <td>nan</td>\n",
       "      <td>2019 – Feb 2020</td>\n",
       "      <td>State University of New York at Binghamton</td>\n",
       "      <td>Alper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Venkat Nelluri</td>\n",
       "      <td>/in/vnelluri</td>\n",
       "      <td>Data Engineer at Albertsons Companies - I'm r...</td>\n",
       "      <td>Dallas-Fort Worth Metroplex</td>\n",
       "      <td>Data Engineer at Albertsons Companies</td>\n",
       "      <td>https://www.linkedin.com/in/vnelluri/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Caris Life Sciences Full-time</td>\n",
       "      <td>Phoenix, Arizona, United States</td>\n",
       "      <td>2021 – Present</td>\n",
       "      <td>Data Science Fellow</td>\n",
       "      <td>The Data Incubator Full-time</td>\n",
       "      <td>Online</td>\n",
       "      <td>2020 – Jun 2020</td>\n",
       "      <td>The Data Incubator</td>\n",
       "      <td>Venkat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rachel Kiesling</td>\n",
       "      <td>/in/rachelkiesling</td>\n",
       "      <td>Data Science Program at Thinkful - - Learned ...</td>\n",
       "      <td>Waltham, MA</td>\n",
       "      <td>Data Engineer at Cognizant</td>\n",
       "      <td>https://www.linkedin.com/in/rachelkiesling/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>COFENSE Full-time</td>\n",
       "      <td>Leesburg, Virginia, United States</td>\n",
       "      <td>2020 – Present</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Technomics, Inc. Full-time</td>\n",
       "      <td>Arlington, Virginia, United States</td>\n",
       "      <td>2019 – Feb 2020</td>\n",
       "      <td>Schreyer Honors College at The Pennsylvania St...</td>\n",
       "      <td>Rachel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Asfetaw Abera</td>\n",
       "      <td>/in/asfetaw-abera-54b0a2185</td>\n",
       "      <td>Data Science Fellow at The Data Incubator - C...</td>\n",
       "      <td>Silver Spring, MD</td>\n",
       "      <td>Data Engineer at Caris Life Sciences</td>\n",
       "      <td>https://www.linkedin.com/in/asfetaw-abera-54b0...</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Verizon Contract</td>\n",
       "      <td>Remote</td>\n",
       "      <td>2020 – Present</td>\n",
       "      <td>Intern, Analytics and Information Management</td>\n",
       "      <td>Amtrak Internship</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "      <td>2019 – Apr 2020</td>\n",
       "      <td>George Mason University</td>\n",
       "      <td>Asfetaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Elena Cadenas</td>\n",
       "      <td>/in/elena-cadenas-378285126</td>\n",
       "      <td>Data Engineer at COFENSE - Develop queries to...</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Data Engineer at COFENSE</td>\n",
       "      <td>https://www.linkedin.com/in/elena-cadenas-3782...</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Capital One Full-time</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "      <td>2020 – Present</td>\n",
       "      <td>Data Software Engineer</td>\n",
       "      <td>The Advisory Board Company</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "      <td>2019 – Feb 2020</td>\n",
       "      <td>University of Maryland</td>\n",
       "      <td>Elena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sruthi Pusuluri</td>\n",
       "      <td>/in/sruthipusuluri</td>\n",
       "      <td>Machine Learning Engineer Intern at NaturalText</td>\n",
       "      <td>Fairfax, VA</td>\n",
       "      <td>Data Engineer at Verizon</td>\n",
       "      <td>https://www.linkedin.com/in/sruthipusuluri/</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Vanguard</td>\n",
       "      <td>Malvern, Pennsylvania</td>\n",
       "      <td>2019 – Present</td>\n",
       "      <td>Company Name\\nThe Johns Hopkins University</td>\n",
       "      <td>nan</td>\n",
       "      <td>Baltimore, Maryland, United States</td>\n",
       "      <td>2019 – Jun 2019</td>\n",
       "      <td>The Johns Hopkins University</td>\n",
       "      <td>Sruthi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rui Ponte</td>\n",
       "      <td>/in/rui-ponte-1a41a4b1</td>\n",
       "      <td>Data Software Engineer at The Advisory Board ...</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>Data Engineer at Capital One</td>\n",
       "      <td>https://www.linkedin.com/in/rui-ponte-1a41a4b1/</td>\n",
       "      <td>Manager, BI/Dashboard Development</td>\n",
       "      <td>Hilton Full-time</td>\n",
       "      <td>Mclean, Virginia</td>\n",
       "      <td>2018 – Present</td>\n",
       "      <td>Senior Business Intelligence Architect</td>\n",
       "      <td>Invexer Technology Inc. Full-time</td>\n",
       "      <td>Washington D.C. Metro Area</td>\n",
       "      <td>2015 – Aug 2018</td>\n",
       "      <td>Georgia Institute of Technology</td>\n",
       "      <td>Rui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Meng Zhao</td>\n",
       "      <td>/in/mzhao15</td>\n",
       "      <td>Data Engineering Fellow at Insight Data Science</td>\n",
       "      <td>Baltimore, MD</td>\n",
       "      <td>Data Engineer at Vanguard</td>\n",
       "      <td>https://www.linkedin.com/in/mzhao15/</td>\n",
       "      <td>Hadoop Developer</td>\n",
       "      <td>DISH Network Full-time</td>\n",
       "      <td>Denver, Colorado, United States</td>\n",
       "      <td>2019 – Present</td>\n",
       "      <td>Big data engineer</td>\n",
       "      <td>Choice Hotels International Full-time</td>\n",
       "      <td>Guayaquil, Guayas, Ecuador</td>\n",
       "      <td>2018 – Aug 2019</td>\n",
       "      <td>Universidad Técnica Particular de Loja</td>\n",
       "      <td>Meng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yisong Li</td>\n",
       "      <td>/in/yisong-li</td>\n",
       "      <td>Manager, BI/Dashboard Development at Hilton -...</td>\n",
       "      <td>Washington DC-Baltimore Area</td>\n",
       "      <td>BI/Data Engineer</td>\n",
       "      <td>https://www.linkedin.com/in/yisong-li/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yisong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Carlos Llerena</td>\n",
       "      <td>/in/carlos-llerena-2232291a8</td>\n",
       "      <td>Hadoop Developer at DISH Network - ...methodo...</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Big data engineer</td>\n",
       "      <td>https://www.linkedin.com/in/carlos-llerena-223...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carlos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                           url  \\\n",
       "0   Murali Subramanian                /in/musubr0298   \n",
       "1        Jianfang Chen   /in/jianfang-chen-2286a31b1   \n",
       "2             Vina Zhu        /in/vina-zhu-4986451a4   \n",
       "3           Palak Kaur                /in/palak-kaur   \n",
       "4          Alper Tosun      /in/alper-tosun-bba82467   \n",
       "5       Venkat Nelluri                  /in/vnelluri   \n",
       "6      Rachel Kiesling            /in/rachelkiesling   \n",
       "7        Asfetaw Abera   /in/asfetaw-abera-54b0a2185   \n",
       "8        Elena Cadenas   /in/elena-cadenas-378285126   \n",
       "9      Sruthi Pusuluri            /in/sruthipusuluri   \n",
       "10           Rui Ponte        /in/rui-ponte-1a41a4b1   \n",
       "11           Meng Zhao                   /in/mzhao15   \n",
       "12           Yisong Li                 /in/yisong-li   \n",
       "13      Carlos Llerena  /in/carlos-llerena-2232291a8   \n",
       "\n",
       "                                          current_job  \\\n",
       "0                         Data Engineer at Searce Inc   \n",
       "1                                    Data Engineering   \n",
       "2                       Data Engineer at Slalom Build   \n",
       "3    A Data Analyst and Engineer taking care of da...   \n",
       "4    Data Management Analyst at Fannie Mae - ...th...   \n",
       "5    Data Engineer at Albertsons Companies - I'm r...   \n",
       "6    Data Science Program at Thinkful - - Learned ...   \n",
       "7    Data Science Fellow at The Data Incubator - C...   \n",
       "8    Data Engineer at COFENSE - Develop queries to...   \n",
       "9     Machine Learning Engineer Intern at NaturalText   \n",
       "10   Data Software Engineer at The Advisory Board ...   \n",
       "11    Data Engineering Fellow at Insight Data Science   \n",
       "12   Manager, BI/Dashboard Development at Hilton -...   \n",
       "13   Hadoop Developer at DISH Network - ...methodo...   \n",
       "\n",
       "                        location  \\\n",
       "0                     Austin, TX   \n",
       "1                  United States   \n",
       "2                  Arlington, VA   \n",
       "3           Greater Seattle Area   \n",
       "4   Washington DC-Baltimore Area   \n",
       "5    Dallas-Fort Worth Metroplex   \n",
       "6                    Waltham, MA   \n",
       "7              Silver Spring, MD   \n",
       "8                  Chantilly, VA   \n",
       "9                    Fairfax, VA   \n",
       "10  Washington DC-Baltimore Area   \n",
       "11                 Baltimore, MD   \n",
       "12  Washington DC-Baltimore Area   \n",
       "13                 Chantilly, VA   \n",
       "\n",
       "                                          headline  \\\n",
       "0   Data Engineer at Searce Inc. | 2x GCP | 2x AWS   \n",
       "1                       Data Engineer at Microsoft   \n",
       "2                                    Data Engineer   \n",
       "3                          Data Engineer at Square   \n",
       "4                      Data Engineer at Fannie Mae   \n",
       "5            Data Engineer at Albertsons Companies   \n",
       "6                       Data Engineer at Cognizant   \n",
       "7             Data Engineer at Caris Life Sciences   \n",
       "8                         Data Engineer at COFENSE   \n",
       "9                         Data Engineer at Verizon   \n",
       "10                    Data Engineer at Capital One   \n",
       "11                       Data Engineer at Vanguard   \n",
       "12                                BI/Data Engineer   \n",
       "13                               Big data engineer   \n",
       "\n",
       "                                                fetch  \\\n",
       "0             https://www.linkedin.com/in/musubr0298/   \n",
       "1   https://www.linkedin.com/in/jianfang-chen-2286...   \n",
       "2     https://www.linkedin.com/in/vina-zhu-4986451a4/   \n",
       "3             https://www.linkedin.com/in/palak-kaur/   \n",
       "4   https://www.linkedin.com/in/alper-tosun-bba82467/   \n",
       "5               https://www.linkedin.com/in/vnelluri/   \n",
       "6         https://www.linkedin.com/in/rachelkiesling/   \n",
       "7   https://www.linkedin.com/in/asfetaw-abera-54b0...   \n",
       "8   https://www.linkedin.com/in/elena-cadenas-3782...   \n",
       "9         https://www.linkedin.com/in/sruthipusuluri/   \n",
       "10    https://www.linkedin.com/in/rui-ponte-1a41a4b1/   \n",
       "11               https://www.linkedin.com/in/mzhao15/   \n",
       "12             https://www.linkedin.com/in/yisong-li/   \n",
       "13  https://www.linkedin.com/in/carlos-llerena-223...   \n",
       "\n",
       "                                 job1                       company1  \\\n",
       "0                       Data Engineer         Slalom Build Full-time   \n",
       "1                       Data Engineer                         Square   \n",
       "2            Company Name\\nFannie Mae                            nan   \n",
       "3                       Data Engineer           Albertsons Companies   \n",
       "4                       Data Engineer            Cognizant Full-time   \n",
       "5                       Data Engineer  Caris Life Sciences Full-time   \n",
       "6                       Data Engineer              COFENSE Full-time   \n",
       "7                       Data Engineer               Verizon Contract   \n",
       "8                       Data Engineer          Capital One Full-time   \n",
       "9                       Data Engineer                       Vanguard   \n",
       "10  Manager, BI/Dashboard Development               Hilton Full-time   \n",
       "11                   Hadoop Developer         DISH Network Full-time   \n",
       "12                                NaN                            NaN   \n",
       "13                                NaN                            NaN   \n",
       "\n",
       "                            location1          dates1  \\\n",
       "0    Chicago, Illinois, United States  2021 – Present   \n",
       "1                                 nan  2021 – Present   \n",
       "2     Reston, Virginia, United States  2020 – Present   \n",
       "3                Greater Phoenix Area  2021 – Present   \n",
       "4                                 nan  2020 – Present   \n",
       "5     Phoenix, Arizona, United States  2021 – Present   \n",
       "6   Leesburg, Virginia, United States  2020 – Present   \n",
       "7                              Remote  2020 – Present   \n",
       "8          Washington D.C. Metro Area  2020 – Present   \n",
       "9               Malvern, Pennsylvania  2019 – Present   \n",
       "10                   Mclean, Virginia  2018 – Present   \n",
       "11    Denver, Colorado, United States  2019 – Present   \n",
       "12                                NaN             NaN   \n",
       "13                                NaN             NaN   \n",
       "\n",
       "                                            job2  \\\n",
       "0                      Machine Learning Engineer   \n",
       "1                      Company Name\\nZoomSystems   \n",
       "2                               Assitant Manager   \n",
       "3                             Company Name\\nCiti   \n",
       "4                           Data Science Program   \n",
       "5                            Data Science Fellow   \n",
       "6                                   Data Analyst   \n",
       "7   Intern, Analytics and Information Management   \n",
       "8                         Data Software Engineer   \n",
       "9     Company Name\\nThe Johns Hopkins University   \n",
       "10        Senior Business Intelligence Architect   \n",
       "11                             Big data engineer   \n",
       "12                                           NaN   \n",
       "13                                           NaN   \n",
       "\n",
       "                                 company2                           location2  \\\n",
       "0                      BrainCo Internship                                 nan   \n",
       "1                                     nan              San Francisco Bay Area   \n",
       "2               Papa John's International                                 nan   \n",
       "3                                     nan        Irving, Texas, United States   \n",
       "4                                Thinkful                                 nan   \n",
       "5            The Data Incubator Full-time                              Online   \n",
       "6              Technomics, Inc. Full-time  Arlington, Virginia, United States   \n",
       "7                       Amtrak Internship          Washington D.C. Metro Area   \n",
       "8              The Advisory Board Company          Washington D.C. Metro Area   \n",
       "9                                     nan  Baltimore, Maryland, United States   \n",
       "10      Invexer Technology Inc. Full-time          Washington D.C. Metro Area   \n",
       "11  Choice Hotels International Full-time          Guayaquil, Guayas, Ecuador   \n",
       "12                                    NaN                                 NaN   \n",
       "13                                    NaN                                 NaN   \n",
       "\n",
       "             dates2                                             school  \\\n",
       "0   2020 – Feb 2021                   The George Washington University   \n",
       "1   2019 – Feb 2021                 Hult International Business School   \n",
       "2   2014 – Jul 2016                           James Madison University   \n",
       "3   2020 – Jan 2021                       Southern Arkansas University   \n",
       "4   2019 – Feb 2020         State University of New York at Binghamton   \n",
       "5   2020 – Jun 2020                                 The Data Incubator   \n",
       "6   2019 – Feb 2020  Schreyer Honors College at The Pennsylvania St...   \n",
       "7   2019 – Apr 2020                            George Mason University   \n",
       "8   2019 – Feb 2020                             University of Maryland   \n",
       "9   2019 – Jun 2019                       The Johns Hopkins University   \n",
       "10  2015 – Aug 2018                    Georgia Institute of Technology   \n",
       "11  2018 – Aug 2019             Universidad Técnica Particular de Loja   \n",
       "12              NaN                                                NaN   \n",
       "13              NaN                                                NaN   \n",
       "\n",
       "   first_name  \n",
       "0      Murali  \n",
       "1    Jianfang  \n",
       "2        Vina  \n",
       "3       Palak  \n",
       "4       Alper  \n",
       "5      Venkat  \n",
       "6      Rachel  \n",
       "7     Asfetaw  \n",
       "8       Elena  \n",
       "9      Sruthi  \n",
       "10        Rui  \n",
       "11       Meng  \n",
       "12     Yisong  \n",
       "13     Carlos  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['personalized_message'] = \"Hi \" + df['first_name']+\", my name is Brennan, and I am a data scientist looking to segue into data engineering. I see that you are a \"+df['job1']+ \" at \" +df['company1']+\". I am super interested in finding out more about what you do in your current position, so I just wanted to reach out, connect, and say hello!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_input = list(df['personalized_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for url in profile_urls:\n",
    "    driver.get(url)\n",
    "    message = str(message_input[counter])\n",
    "    profile_connect(message)\n",
    "    \n",
    "\n",
    "    counter+=1\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.forward()\n",
    "driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_urls = df['fetch'][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                         Data Engineer\n",
      "1                         Data Engineer\n",
      "2                         Data Engineer\n",
      "3                         Data Engineer\n",
      "4              Company Name\\nFannie Mae\n",
      "5                         Data Engineer\n",
      "6                         Data Engineer\n",
      "7                         Data Engineer\n",
      "8                         Data Engineer\n",
      "9                         Data Engineer\n",
      "10                        Data Engineer\n",
      "11                        Data Engineer\n",
      "12    Manager, BI/Dashboard Development\n",
      "13                     Hadoop Developer\n",
      "Name: job1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.job1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
